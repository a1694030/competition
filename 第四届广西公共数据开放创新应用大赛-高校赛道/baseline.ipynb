{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# 读取数据\n",
    "# 训练集\n",
    "data_1 =  pd.read_excel('./data/train/data1/Part_1.xlsx')\n",
    "data_2 =  pd.read_excel('./data/train/data2/Part_2.xlsx')\n",
    "data_3 =  pd.read_excel('./data/train/data3/Part_3.xlsx')\n",
    "data_4 =  pd.read_excel('./data/train/data4/Part_4.xlsx')\n",
    "data_5 =  pd.read_excel('./data/train/data5/Part_5.xlsx')\n",
    "data_6 =  pd.read_excel('./data/train/data6/Part_6.xlsx')\n",
    "# data_7 =  pd.read_excel('./data/train/data7/Part_7.xlsx')\n",
    "# 合并\n",
    "train_df = pd.concat([data_1,data_2,data_3,data_4,data_5,data_6],ignore_index=True)\n",
    "\n",
    "# 测试集\n",
    "test_df = pd.read_excel('./data/test/test.xlsx')\n",
    "\n",
    "# 合并训练集，测试集\n",
    "train_df['is_test'] = 0\n",
    "test_df['is_test'] = 1\n",
    "df_all = pd.concat([train_df,test_df],axis=0,ignore_index=True)\n",
    "\n",
    "# # 文本清洗\n",
    "# def clean_text(text):\n",
    "#     # 去除标点符号和数字\n",
    "#     # text = re.sub(r'[^\\u4e00-\\u9fa5]+', '', text)\n",
    "#     # 去掉文本中的空格\n",
    "#     text = text.replace(' ','') \n",
    "#     text = text.replace('...',',') \n",
    "#     # text = text.replace('【',' ') \n",
    "#     # text = text.replace('】',',')\n",
    "#     # text = text.replace('[',' ') \n",
    "#     # text = text.replace(']',',')\n",
    "\n",
    "#     return text\n",
    "# df_all['text'] = df_all['text'].apply(lambda x:clean_text(x))\n",
    "\n",
    "#文本特征\n",
    "tfidf = TfidfVectorizer(max_features=16000)\n",
    "tfidf_df = tfidf.fit_transform(df_all['text'])\n",
    "tfidf_feat = pd.DataFrame(tfidf_df.toarray(),columns=tfidf.get_feature_names_out())\n",
    "\n",
    "# #图像特征\n",
    "# transform = transforms.Compose([\n",
    "#     transforms.Resize((224, 224)),\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "# ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feats_num: 16000\n"
     ]
    }
   ],
   "source": [
    "# 构造\n",
    "tfidf_feat['id'] = df_all['id']\n",
    "tfidf_feat['label'] = df_all['label']\n",
    "tfidf_feat['is_test'] = df_all['is_test']\n",
    "df_all = tfidf_feat\n",
    "df_all = df_all.fillna(0)\n",
    "\n",
    "# 分离训练集，测试集\n",
    "train_df = df_all[df_all['is_test']==0]\n",
    "test_df = df_all[df_all['is_test']==1]\n",
    "\n",
    "feats = [col for col in test_df if col not in ['id','label','is_test']]                                                                                                                                                                                      \n",
    "print('feats_num:',len(feats))\n",
    "\n",
    "del tfidf_df,tfidf_feat,df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Memory usage is: 2986.34 MB\n",
      "Memory usage after optimization is: 746.75 MB\n",
      "Decreased by 75.0%\n",
      "Start Memory usage is: 1425.77 MB\n",
      "Memory usage after optimization is: 356.52 MB\n",
      "Decreased by 75.0%\n"
     ]
    }
   ],
   "source": [
    "#内存压缩\n",
    "import time\n",
    "pd.set_option('mode.chained_assignment', None)\n",
    "def reduce_mem_usage(df):\n",
    "    starttime = time.time()\n",
    "    numerics = ['int16','int32','int64','float16','float32 ','float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    for col in df.columns:\n",
    "        col_type = df[ col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[ col].max()\n",
    "            if pd.isnull(c_min) or pd.isnull(c_max):\n",
    "                continue\n",
    "            if str(col_type)[:3] == 'int' :\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8 ).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64 ).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype( np.float64)\n",
    "    end_mem = df.memory_usage( ).sum() / 1024**2\n",
    "    print('Start Memory usage is: {:.2f} MB'.format(start_mem))\n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "\n",
    "reduce_mem_usage(train_df)\n",
    "reduce_mem_usage(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 2.33 GiB for an array with shape (19566, 16000) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"d:\\anaconda\\envs\\machine_learning_39\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 463, in _process_worker\n    r = call_item()\n  File \"d:\\anaconda\\envs\\machine_learning_39\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 291, in __call__\n    return self.fn(*self.args, **self.kwargs)\n  File \"d:\\anaconda\\envs\\machine_learning_39\\lib\\site-packages\\joblib\\parallel.py\", line 598, in __call__\n    return [func(*args, **kwargs)\n  File \"d:\\anaconda\\envs\\machine_learning_39\\lib\\site-packages\\joblib\\parallel.py\", line 598, in <listcomp>\n    return [func(*args, **kwargs)\n  File \"d:\\anaconda\\envs\\machine_learning_39\\lib\\site-packages\\sklearn\\utils\\parallel.py\", line 129, in __call__\n    return self.function(*args, **kwargs)\n  File \"d:\\anaconda\\envs\\machine_learning_39\\lib\\site-packages\\sklearn\\ensemble\\_base.py\", line 40, in _fit_single_estimator\n    estimator.fit(X, y, **fit_params)\n  File \"d:\\anaconda\\envs\\machine_learning_39\\lib\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n  File \"d:\\anaconda\\envs\\machine_learning_39\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py\", line 938, in fit\n    return self._fit(\n  File \"d:\\anaconda\\envs\\machine_learning_39\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py\", line 725, in _fit\n    self._partial_fit(\n  File \"d:\\anaconda\\envs\\machine_learning_39\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py\", line 596, in _partial_fit\n    X, y = self._validate_data(\n  File \"d:\\anaconda\\envs\\machine_learning_39\\lib\\site-packages\\sklearn\\base.py\", line 650, in _validate_data\n    X, y = check_X_y(X, y, **check_params)\n  File \"d:\\anaconda\\envs\\machine_learning_39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 1273, in check_X_y\n    X = check_array(\n  File \"d:\\anaconda\\envs\\machine_learning_39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 1007, in check_array\n    array = _asarray_with_order(array, order=order, dtype=dtype, xp=xp)\n  File \"d:\\anaconda\\envs\\machine_learning_39\\lib\\site-packages\\sklearn\\utils\\_array_api.py\", line 746, in _asarray_with_order\n    array = numpy.asarray(array, order=order, dtype=dtype)\nnumpy.core._exceptions._ArrayMemoryError: Unable to allocate 2.33 GiB for an array with shape (19566, 16000) and data type float64\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[174], line 75\u001b[0m\n\u001b[0;32m     73\u001b[0m target_col \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     74\u001b[0m cv \u001b[38;5;241m=\u001b[39m KFold(\u001b[38;5;241m5\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m9999\u001b[39m)  \u001b[38;5;66;03m#StratifiedGroupKFold\u001b[39;00m\n\u001b[1;32m---> 75\u001b[0m val_scores, test_preds, oof_preds \u001b[38;5;241m=\u001b[39m \u001b[43mcross_validate_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mensemble_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfeats\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtarget_col\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_df\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfeats\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[174], line 46\u001b[0m, in \u001b[0;36mcross_validate_score\u001b[1;34m(model, train_df, y, cv, test_df)\u001b[0m\n\u001b[0;32m     43\u001b[0m X_train, y_train \u001b[38;5;241m=\u001b[39m train_df\u001b[38;5;241m.\u001b[39miloc[train_idx]\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m), y\u001b[38;5;241m.\u001b[39miloc[train_idx]\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     44\u001b[0m X_val, y_val \u001b[38;5;241m=\u001b[39m train_df\u001b[38;5;241m.\u001b[39miloc[val_idx]\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m), y\u001b[38;5;241m.\u001b[39miloc[val_idx]\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m---> 46\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     48\u001b[0m val_probs \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict_proba(X_val)[:, \u001b[38;5;241m1\u001b[39m]  \n\u001b[0;32m     49\u001b[0m val_preds \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mwhere(val_probs\u001b[38;5;241m>\u001b[39m\u001b[38;5;241m0.5\u001b[39m,\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32md:\\anaconda\\envs\\machine_learning_39\\lib\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\anaconda\\envs\\machine_learning_39\\lib\\site-packages\\sklearn\\utils\\validation.py:69\u001b[0m, in \u001b[0;36m_deprecate_positional_args.<locals>._inner_deprecate_positional_args.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m extra_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(all_args)\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m extra_args \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m---> 69\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     71\u001b[0m \u001b[38;5;66;03m# extra_args > 0\u001b[39;00m\n\u001b[0;32m     72\u001b[0m args_msg \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     73\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(name, arg)\n\u001b[0;32m     74\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m name, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(kwonly_args[:extra_args], args[\u001b[38;5;241m-\u001b[39mextra_args:])\n\u001b[0;32m     75\u001b[0m ]\n",
      "File \u001b[1;32md:\\anaconda\\envs\\machine_learning_39\\lib\\site-packages\\sklearn\\ensemble\\_voting.py:423\u001b[0m, in \u001b[0;36mVotingClassifier.fit\u001b[1;34m(self, X, y, sample_weight, **fit_params)\u001b[0m\n\u001b[0;32m    420\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    421\u001b[0m     fit_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msample_weight\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m sample_weight\n\u001b[1;32m--> 423\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mfit(X, transformed_y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n",
      "File \u001b[1;32md:\\anaconda\\envs\\machine_learning_39\\lib\\site-packages\\sklearn\\ensemble\\_voting.py:104\u001b[0m, in \u001b[0;36m_BaseVoting.fit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m     99\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msample_weight\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m fit_params:\n\u001b[0;32m    100\u001b[0m             routed_params[name]\u001b[38;5;241m.\u001b[39mfit[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msample_weight\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m fit_params[\n\u001b[0;32m    101\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msample_weight\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    102\u001b[0m             ]\n\u001b[1;32m--> 104\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_ \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    105\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_single_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    106\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    107\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    108\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    109\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfit_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrouted_params\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfit\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    110\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessage_clsname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mVoting\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    111\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_log_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mclfs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    112\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    113\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnames\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclfs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    114\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mclf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m!=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdrop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m    115\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnamed_estimators_ \u001b[38;5;241m=\u001b[39m Bunch()\n\u001b[0;32m    119\u001b[0m \u001b[38;5;66;03m# Uses 'drop' as placeholder for dropped estimators\u001b[39;00m\n",
      "File \u001b[1;32md:\\anaconda\\envs\\machine_learning_39\\lib\\site-packages\\sklearn\\utils\\parallel.py:67\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     62\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     63\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     64\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     66\u001b[0m )\n\u001b[1;32m---> 67\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\anaconda\\envs\\machine_learning_39\\lib\\site-packages\\joblib\\parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   2001\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[0;32m   2002\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[0;32m   2003\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[0;32m   2004\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[0;32m   2005\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 2007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\anaconda\\envs\\machine_learning_39\\lib\\site-packages\\joblib\\parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[1;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[0;32m   1647\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[0;32m   1649\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1650\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[0;32m   1652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[0;32m   1653\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[0;32m   1654\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[0;32m   1655\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[0;32m   1656\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32md:\\anaconda\\envs\\machine_learning_39\\lib\\site-packages\\joblib\\parallel.py:1754\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait_retrieval():\n\u001b[0;32m   1748\u001b[0m \n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;66;03m# If the callback thread of a worker has signaled that its task\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m     \u001b[38;5;66;03m# triggered an exception, or if the retrieval loop has raised an\u001b[39;00m\n\u001b[0;32m   1751\u001b[0m     \u001b[38;5;66;03m# exception (e.g. `GeneratorExit`), exit the loop and surface the\u001b[39;00m\n\u001b[0;32m   1752\u001b[0m     \u001b[38;5;66;03m# worker traceback.\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_aborting:\n\u001b[1;32m-> 1754\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_error_fast\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1755\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m   1757\u001b[0m     \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m     \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n",
      "File \u001b[1;32md:\\anaconda\\envs\\machine_learning_39\\lib\\site-packages\\joblib\\parallel.py:1789\u001b[0m, in \u001b[0;36mParallel._raise_error_fast\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1785\u001b[0m \u001b[38;5;66;03m# If this error job exists, immediately raise the error by\u001b[39;00m\n\u001b[0;32m   1786\u001b[0m \u001b[38;5;66;03m# calling get_result. This job might not exists if abort has been\u001b[39;00m\n\u001b[0;32m   1787\u001b[0m \u001b[38;5;66;03m# called directly or if the generator is gc'ed.\u001b[39;00m\n\u001b[0;32m   1788\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m error_job \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1789\u001b[0m     \u001b[43merror_job\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_result\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\anaconda\\envs\\machine_learning_39\\lib\\site-packages\\joblib\\parallel.py:745\u001b[0m, in \u001b[0;36mBatchCompletionCallBack.get_result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    739\u001b[0m backend \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparallel\u001b[38;5;241m.\u001b[39m_backend\n\u001b[0;32m    741\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m backend\u001b[38;5;241m.\u001b[39msupports_retrieve_callback:\n\u001b[0;32m    742\u001b[0m     \u001b[38;5;66;03m# We assume that the result has already been retrieved by the\u001b[39;00m\n\u001b[0;32m    743\u001b[0m     \u001b[38;5;66;03m# callback thread, and is stored internally. It's just waiting to\u001b[39;00m\n\u001b[0;32m    744\u001b[0m     \u001b[38;5;66;03m# be returned.\u001b[39;00m\n\u001b[1;32m--> 745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_return_or_raise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    747\u001b[0m \u001b[38;5;66;03m# For other backends, the main thread needs to run the retrieval step.\u001b[39;00m\n\u001b[0;32m    748\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32md:\\anaconda\\envs\\machine_learning_39\\lib\\site-packages\\joblib\\parallel.py:763\u001b[0m, in \u001b[0;36mBatchCompletionCallBack._return_or_raise\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    761\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    762\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m==\u001b[39m TASK_ERROR:\n\u001b[1;32m--> 763\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n\u001b[0;32m    764\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n\u001b[0;32m    765\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 2.33 GiB for an array with shape (19566, 16000) and data type float64"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "# from vote_mode import get_model\n",
    "from sklearn.metrics import f1_score,accuracy_score\n",
    "from sklearn.model_selection import StratifiedKFold,KFold\n",
    "\n",
    "import joblib\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 定义模型\n",
    "def get_model():\n",
    "\n",
    "    # rf = RandomForestClassifier()\n",
    "    # clf2 = MultinomialNB(alpha=0.01)\n",
    "    # clf = MultinomialNB(alpha=0.0225)\n",
    "    # cat = CatBoostClassifier()\n",
    "\n",
    "    rg = LogisticRegression(penalty='l2',tol=1e-4)\n",
    "    sgd = SGDClassifier(n_iter_no_change = 10,max_iter=1000, tol=1e-5, loss=\"modified_huber\", random_state=67654)\n",
    "    lgb = LGBMClassifier(n_estimators=200,num_leaves=256,max_depth=10)\n",
    "\n",
    "    weights = [0.08, 0.9, 0.02]\n",
    "    ensemble = VotingClassifier(estimators=[('rg', rg),\n",
    "                                            ('sgd', sgd),\n",
    "                                            ('lgb', lgb)],\n",
    "                                weights=weights, voting='soft', n_jobs=-1)\n",
    "    return ensemble\n",
    "\n",
    "def cross_validate_score(model, train_df, y, cv, test_df):\n",
    "    val_scores = []\n",
    "    test_preds = np.zeros((test_df.shape[0],))\n",
    "    oof_preds = np.zeros((train_df.shape[0],))\n",
    "\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(cv.split(train_df, y)):\n",
    "\n",
    "        X_train, y_train = train_df.iloc[train_idx].reset_index(drop=True), y.iloc[train_idx].reset_index(drop=True)\n",
    "        X_val, y_val = train_df.iloc[val_idx].reset_index(drop=True), y.iloc[val_idx].reset_index(drop=True)\n",
    "        \n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        val_probs = model.predict_proba(X_val)[:, 1]  \n",
    "        val_preds = np.where(val_probs>0.5,1,0)\n",
    "\n",
    "        val_score = f1_score(val_preds, y_val, average='weighted')  \n",
    "        print(f'Fold {fold+1}: score = {val_score:.5f}')\n",
    "        \n",
    "        val_scores.append(val_score)\n",
    "        \n",
    "        oof_preds[val_idx] = val_probs  \n",
    "\n",
    "        test_preds += model.predict_proba(test_df)[:, 1] / cv.get_n_splits()  \n",
    "\n",
    "    mean_val_score = np.mean(val_scores)\n",
    "    std_val_score = np.std(val_scores)\n",
    "    print(f'Mean Validation score: {mean_val_score:.7f}')\n",
    "    print(f'Std Validation score: {std_val_score:.7f}')\n",
    "\n",
    "    del X_train,y_train,X_val,y_val,model\n",
    "    \n",
    "    gc.collect()\n",
    "\n",
    "    return val_scores, test_preds, oof_preds\n",
    "\n",
    "# 训练\n",
    "ensemble_model = get_model()\n",
    "target_col = 'label'\n",
    "cv = KFold(5, shuffle=True, random_state=9999)  #StratifiedGroupKFold\n",
    "val_scores, test_preds, oof_preds = cross_validate_score(ensemble_model, train_df[feats], train_df[target_col], cv, test_df[feats])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 融合\n",
    "# predictions_prob_dl_oof =  pd.read_csv('predictions_prob_dl_oof.csv')\n",
    "predictions_prob_dl_test =  pd.read_csv('predictions_prob_dl_test_0.9003.csv')\n",
    "predictions_prob_dl_test2 =  pd.read_csv('predictions_prob_dl_test_0.8993.csv')\n",
    "\n",
    "# preds = predictions_prob_dl['target']*0.8 + test_preds*0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型权重搜索\n",
    "scores = []; Weights = []\n",
    "model_best_m = 0\n",
    "best_score = 0\n",
    "for m in np.arange(0.1,0.9,0.01):\n",
    "    enb_pre = predictions_prob_dl_oof['target']*m + oof_preds*(1-m)\n",
    "    score = f1_score(np.where(enb_pre>0.5,1,0), train_df[target_col], average='weighted')\n",
    "    scores.append(score)\n",
    "    Weights.append(m)\n",
    "    if score > best_score:\n",
    "        best_score = score\n",
    "        model_best_m = m\n",
    "print(f'best_score : {best_score}   best_m : {model_best_m}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 指标权重搜索\n",
    "scores = []; Weights = []\n",
    "f1_best_m = 0\n",
    "best_score = 0\n",
    "for m in np.arange(0.1,0.9,0.01):\n",
    "    pres = predictions_prob_dl_oof['target']\n",
    "    score = f1_score(np.where(pres>m,1,0), train_df[target_col], average='weighted')\n",
    "    scores.append(score)\n",
    "    Weights.append(m)\n",
    "    if score > best_score:\n",
    "        best_score = score\n",
    "        f1_best_m = m\n",
    "print(f'best_score : {best_score}   best_m : {f1_best_m}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 融合\n",
    "# 第一层\n",
    "pres = predictions_prob_dl_test['target']*0.6 + predictions_prob_dl_test2['target']*0.4\n",
    "# 第二层\n",
    "final_pres = pres*0.47 + test_preds*0.53\n",
    "test_label = np.where(final_pres>0.5,1,0)\n",
    "# 保存\n",
    "predictions = pd.DataFrame({'id':test_df['id'],'target':test_label})\n",
    "predictions.to_csv('predictions.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 投票融合\n",
    "predictions1 =  pd.read_csv('./ensemble_sub/predictions_bert_enb_prob_test.csv')\n",
    "predictions2 =  pd.read_csv('./ensemble_sub/predictions_bert_lgb_test.csv')\n",
    "predictions3 =  pd.read_csv('./ensemble_sub/predictions_prob_dl_test_0.8993.csv')\n",
    "predictions3['target'] = np.where(predictions3['target']>0.4,1,0)\n",
    "predictions4 =  pd.read_csv('./ensemble_sub/predictions_prob_dl_test_0.9003.csv')\n",
    "predictions4['target'] = np.where(predictions4['target']>0.4,1,0)\n",
    "predictions5 =  pd.read_csv('./ensemble_sub/predictions_score0.8562.csv')\n",
    "predictions6 =  pd.read_csv('./ensemble_sub/predictions_score0.9061.csv')\n",
    "predictions7 =  pd.read_csv('./ensemble_sub/predictions_score0.9171.csv')\n",
    "predictions8 =  pd.read_csv('./ensemble_sub/predictions_score0.9275.csv')\n",
    "predictions9 =  pd.read_csv('./ensemble_sub/predictions_score0.9294.csv')\n",
    "predictions10 =  pd.read_csv('./ensemble_sub/predictions_score0.9306.csv')\n",
    "predictions11 =  pd.read_csv('./ensemble_sub/predictions_score0.9323.csv')\n",
    "predictions12 =  pd.read_csv('./ensemble_sub/predictions_score0.9387.csv')\n",
    "predictions13 =  pd.read_csv('./ensemble_sub/predictions_score0.9402.csv')\n",
    "predictions14 =  pd.read_csv('./ensemble_sub/predictions_score0.9411.csv')\n",
    "predictions15 =  pd.read_csv('./ensemble_sub/predictions_score0.9418.csv')\n",
    "predictions16 =  pd.read_csv('./ensemble_sub/predictions_score0.9422.csv')\n",
    "predictions17 =  pd.read_csv('./ensemble_sub/predictions_score0.934.csv')\n",
    "predictions18 =  pd.read_csv('./sub/predictions_0.9038.csv')\n",
    "predictions19 =  pd.read_csv('./sub/predictions_0.9042.csv')\n",
    "predictions20=  pd.read_csv('./sub/predictions_0.9043.csv')\n",
    "predictions21=  pd.read_csv('./sub/predictions_0.9061.csv')\n",
    "pres_df = pd.DataFrame({\n",
    "                        'predictions5':predictions5['target'],'predictions6':predictions6['target'],\n",
    "                        'predictions7':predictions7['target'],'predictions8':predictions8['target'],\n",
    "                        'predictions9':predictions9['target'],'predictions10':predictions10['target'],\n",
    "                        'predictions11':predictions11['target'],'predictions12':predictions12['target'],\n",
    "                        'predictions15':predictions15['target'],'predictions16':predictions16['target'],\n",
    "                        'predictions17':predictions17['target'],'predictions18':predictions18['target'],\n",
    "                        'predictions19':predictions19['target'],'predictions20':predictions20['target'],\n",
    "                        'predictions21':predictions21['target']                                            })\n",
    "\n",
    "preds = pres_df.mode(axis=1)[0].values\n",
    "\n",
    "# 保存\n",
    "predictions = pd.DataFrame({'id':test_df['id'].values,'target':preds.astype('int')})\n",
    "predictions.to_csv('predictions.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>402</td>\n",
       "      <td>0.718481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30953</td>\n",
       "      <td>0.998661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>21387</td>\n",
       "      <td>0.996258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>39304</td>\n",
       "      <td>0.999024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>25836</td>\n",
       "      <td>0.019439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11672</th>\n",
       "      <td>9635</td>\n",
       "      <td>0.999719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11673</th>\n",
       "      <td>5435</td>\n",
       "      <td>0.994297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11674</th>\n",
       "      <td>8040</td>\n",
       "      <td>0.002317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11675</th>\n",
       "      <td>25891</td>\n",
       "      <td>0.074487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11676</th>\n",
       "      <td>14372</td>\n",
       "      <td>0.006836</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11677 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id    target\n",
       "0        402  0.718481\n",
       "1      30953  0.998661\n",
       "2      21387  0.996258\n",
       "3      39304  0.999024\n",
       "4      25836  0.019439\n",
       "...      ...       ...\n",
       "11672   9635  0.999719\n",
       "11673   5435  0.994297\n",
       "11674   8040  0.002317\n",
       "11675  25891  0.074487\n",
       "11676  14372  0.006836\n",
       "\n",
       "[11677 rows x 2 columns]"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine_learning_39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
