{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.utils import shuffle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#读取数据\n",
    "train_df = np.load(\"./data/训练集/train_x.npy\")\n",
    "train_y = np.load(\"./data/训练集/train_y.npy\")\n",
    "test_df = np.load(\"./data/测试集A/test_x_A.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_feat = pd.read_csv('./data/train_feat.csv')\n",
    "test_feat = pd.read_csv('./data/test_feat.csv')\n",
    "\n",
    "cao_train_feat = pd.read_excel('./cao/feature/feature_train.xlsx')\n",
    "cao_test_feat = pd.read_excel('./cao/feature/feature_test.xlsx')\n",
    "cao_train_feat.drop(columns='label',inplace=True)\n",
    "\n",
    "cao_train_feat.columns = cao_train_feat.columns + '_cao'\n",
    "cao_test_feat.columns = cao_test_feat.columns + '_cao'\n",
    "\n",
    "train_feat['label'] = train_y\n",
    "\n",
    "train_feat.shape,test_feat.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#样本平衡\n",
    "train_df = train_feat[train_feat.label==0][:5000]._append(train_feat[train_feat.label!=0]).reset_index(drop=True)\n",
    "train_y = train_df['label']\n",
    "train_df = train_df.drop(columns='label')\n",
    "test_df = test_feat.reset_index(drop=True)\n",
    "\n",
    "#合并特征\n",
    "train_df = pd.concat([train_df,cao_train_feat],axis=1)\n",
    "test_df = pd.concat([test_df,cao_test_feat],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#对抗验证\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#分离训练集，测试集\n",
    "\n",
    "train_df['is_test'] = 0\n",
    "test_df['is_test'] = 1\n",
    "feat = [col for col in train_df if col not in ['label','is_test']]\n",
    "print('筛选前 feat_num:',len(feat))\n",
    "\n",
    "\n",
    "def Adversarial_val(train_df,test_df,feat_cols,lable):\n",
    "    #合并\n",
    "    df = pd.concat([train_df,test_df],axis=0,ignore_index=True)\n",
    "    df = shuffle(df,random_state=16)\n",
    "    # model = lgb.LGBMClassifier(metric='auc',verbose=-1).fit(df[feat_cols],df[lable])\n",
    "    model = xgb.XGBClassifier(n_estimators=100,eval_metric= 'auc').fit(df[feat_cols],df[lable])\n",
    "    #imp\n",
    "    feat_imp_df = pd.DataFrame({'feature':feat_cols,'imp':np.zeros(len(feat_cols))})\n",
    "    imp = model.feature_importances_\n",
    "    feat_imp_df['imp'] = imp\n",
    "    feat_imp_df = feat_imp_df.sort_values(by='imp', ascending=False).reset_index(drop=True)\n",
    "    feat_imp_df['rank'] = range(feat_imp_df.shape[0])\n",
    "    #预测\n",
    "    df_pre = model.predict_proba(df[feat_cols])[:,1]\n",
    "    auc_score = roc_auc_score(df['is_test'],df_pre)\n",
    "    # print('AUC:',auc_score)\n",
    "    return model,df_pre,feat_imp_df,auc_score\n",
    "\n",
    "#特征筛选\n",
    "select_feat = []\n",
    "drop_feat = []\n",
    "\n",
    "for feat in tqdm(feat):\n",
    "    adv_model,df_pre,feat_imp_df,auc_score=Adversarial_val(train_df,test_df,[feat],'is_test')\n",
    "    if auc_score<0.66:\n",
    "        select_feat.append(feat)\n",
    "    else:\n",
    "        drop_feat.append(feat)\n",
    "print('筛选后 feat_num:',len(select_feat))\n",
    "feat_cols = select_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feat_cols = [col for col in train_df if col not in ['is_test','label','is_test_prob']]\n",
    "hr_drop_cols = ['mean_nni_hr', 'sdnn_hr', 'sdsd_hr', 'nni_50_hr', 'pnni_50_hr',\n",
    "       'nni_20_hr', 'pnni_20_hr', 'rmssd_hr', 'median_nni_hr', 'range_nni_hr',\n",
    "       'cvsd_hr', 'cvnni_hr', 'mean_hr_hr', 'max_hr_hr', 'min_hr_hr',\n",
    "       'std_hr_hr']\n",
    "feat_cols =[col for col in select_feat if col not in ['bo_diff_1_mean', 'bo_diff_1_median', 'cvnni_bo', 'nni_50_hr', 'pnni_50_hr']+hr_drop_cols]\n",
    "# feat_cols =select_feat\n",
    "len(feat_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lazypredict.Supervised import LazyRegressor\n",
    "from lazypredict.Supervised import LazyClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "# 定义 X 和 y 变量\n",
    "X = train_df[feat_cols]\n",
    "y = train_y\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# 使用 LazyRegressor 定义模型\n",
    "multiple_ML_model = LazyClassifier(verbose=0,\n",
    "          ignore_warnings=True,\n",
    "          predictions=True)\n",
    "# 对模型进行拟合，同时预测每个模型的输出结果\n",
    "models, predictions = multiple_ML_model.fit(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rf\n",
    "import warnings\n",
    "import lightgbm as lgb\n",
    "# from lightgbm import early_stopping\n",
    "# from lightgbm import log_evaluation\n",
    "from sklearn.ensemble import RandomForestClassifier,ExtraTreesClassifier\n",
    "from sklearn.model_selection import StratifiedKFold,KFold\n",
    "from sklearn.metrics import roc_auc_score,accuracy_score,classification_report,f1_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "def lgb_model(train_x, train_y, test_x):\n",
    "    seeds=[2024]\n",
    "    oof = np.zeros([train_x.shape[0], 3])\n",
    "    test_predict = np.zeros([test_x.shape[0], 3])\n",
    "    feat_imp_df = pd.DataFrame()\n",
    "    feat_imp_df['feature'] = train_x.columns\n",
    "    feat_imp_df['imp'] = 0\n",
    "    for seed in seeds:\n",
    "        print('Seed:',seed)\n",
    "        folds = 10\n",
    "        kf = StratifiedKFold(n_splits=folds, shuffle=True, random_state=seed)\n",
    "        acc_scores = []\n",
    "        # train_x = train_x.values\n",
    "        # train_y = train_y.values\n",
    "        \n",
    "        for i, (train_index, valid_index) in enumerate(kf.split(train_x, train_y)):\n",
    "            print(\"|  Model  Fold  {}  Training Start           |\".format(str(i + 1)))\n",
    "            \n",
    "            trn_x, trn_y, val_x, val_y = train_x.values[train_index], train_y[train_index], train_x.values[valid_index], \\\n",
    "                                        train_y[valid_index] \n",
    "            \n",
    "            params={'n_estimators':500}\n",
    "            model = RandomForestClassifier(**params).fit(trn_x, trn_y)\n",
    "\n",
    "            val_pred  = model.predict_proba(val_x)\n",
    "            test_pred = model.predict_proba(test_x)\n",
    "\n",
    "            #特征重要性\n",
    "            # feat_imp_df['imp'] += lgb_model.feature_importance(importance_type='gain') / folds/ len(seeds)\n",
    "            # feat_imp_df = feat_imp_df.sort_values(by='imp', ascending=False).reset_index(drop=True)\n",
    "            # feat_imp_df['rank'] = range(feat_imp_df.shape[0])\n",
    "            \n",
    "            oof[valid_index] = val_pred/ len(seeds)\n",
    "            test_predict += test_pred / kf.n_splits / len(seeds)\n",
    "            \n",
    "            acc_score = accuracy_score(val_y,np.argmax(val_pred,axis=1))\n",
    "            print(acc_score)\n",
    "            acc_scores.append(acc_score)\n",
    "            print('AVG_acc :',sum(acc_scores)/len(acc_scores))\n",
    "            print('std :',np.std(acc_scores))\n",
    "        print('oof:',classification_report(train_y,np.argmax(oof,axis=1)))\n",
    "        \n",
    "    return oof, test_predict,feat_imp_df\n",
    "\n",
    "# 训练LGB模型\n",
    "rf_oof, rf_test, lgb_imp_df = lgb_model(train_df[feat_cols].fillna(0), train_y, test_df[feat_cols].fillna(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed: 2024\n",
      "|  Model  Fold  1  Training Start           |\n",
      "0.6894117647058824\n",
      "AVG_acc : 0.6894117647058824\n",
      "std : 0.0\n",
      "|  Model  Fold  2  Training Start           |\n",
      "0.7307692307692307\n",
      "AVG_acc : 0.7100904977375566\n",
      "std : 0.020678733031674168\n",
      "|  Model  Fold  3  Training Start           |\n",
      "0.7182103610675039\n",
      "AVG_acc : 0.712797118847539\n",
      "std : 0.017312565634903458\n",
      "|  Model  Fold  4  Training Start           |\n",
      "0.7299843014128728\n",
      "AVG_acc : 0.7170939144888725\n",
      "std : 0.01673860970921052\n",
      "|  Model  Fold  5  Training Start           |\n",
      "0.728414442700157\n",
      "AVG_acc : 0.7193580201311294\n",
      "std : 0.015641276847975664\n",
      "|  Model  Fold  6  Training Start           |\n",
      "0.7362637362637363\n",
      "AVG_acc : 0.7221756394865638\n",
      "std : 0.015606713684280613\n",
      "|  Model  Fold  7  Training Start           |\n",
      "0.706436420722135\n",
      "AVG_acc : 0.719927179663074\n",
      "std : 0.015463094674594386\n",
      "|  Model  Fold  8  Training Start           |\n",
      "0.7095761381475667\n",
      "AVG_acc : 0.7186332994736355\n",
      "std : 0.014863975457911115\n",
      "|  Model  Fold  9  Training Start           |\n",
      "0.7237048665620094\n",
      "AVG_acc : 0.7191968069278993\n",
      "std : 0.014104235211767855\n",
      "|  Model  Fold  10  Training Start           |\n",
      "0.7229199372056515\n",
      "AVG_acc : 0.7195691199556745\n",
      "std : 0.01342698993571699\n",
      "oof:               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.87      0.83      5000\n",
      "           1       0.68      0.53      0.60      3221\n",
      "           2       0.66      0.69      0.68      4520\n",
      "\n",
      "    accuracy                           0.72     12741\n",
      "   macro avg       0.71      0.70      0.70     12741\n",
      "weighted avg       0.72      0.72      0.71     12741\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#ExtraTreesClassifier\n",
    "import warnings\n",
    "import lightgbm as lgb\n",
    "# from lightgbm import early_stopping\n",
    "# from lightgbm import log_evaluation\n",
    "from sklearn.ensemble import RandomForestClassifier,ExtraTreesClassifier\n",
    "from sklearn.model_selection import StratifiedKFold,KFold\n",
    "from sklearn.metrics import roc_auc_score,accuracy_score,classification_report,f1_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "def lgb_model(train_x, train_y, test_x):\n",
    "    seeds=[2024]\n",
    "    oof = np.zeros([train_x.shape[0], 3])\n",
    "    test_predict = np.zeros([test_x.shape[0], 3])\n",
    "    feat_imp_df = pd.DataFrame()\n",
    "    feat_imp_df['feature'] = train_x.columns\n",
    "    feat_imp_df['imp'] = 0\n",
    "    for seed in seeds:\n",
    "        print('Seed:',seed)\n",
    "        folds = 10\n",
    "        kf = StratifiedKFold(n_splits=folds, shuffle=True, random_state=seed)\n",
    "        acc_scores = []\n",
    "        # train_x = train_x.values\n",
    "        # train_y = train_y.values\n",
    "        \n",
    "        for i, (train_index, valid_index) in enumerate(kf.split(train_x, train_y)):\n",
    "            print(\"|  Model  Fold  {}  Training Start           |\".format(str(i + 1)))\n",
    "            \n",
    "            trn_x, trn_y, val_x, val_y = train_x.values[train_index], train_y[train_index], train_x.values[valid_index], \\\n",
    "                                        train_y[valid_index] \n",
    "            \n",
    "            params={'class_weight':'balanced', 'max_depth':25,'min_samples_leaf':50,'n_estimators':500, 'random_state':2024}\n",
    "            model = ExtraTreesClassifier().fit(trn_x, trn_y)\n",
    "\n",
    "            val_pred  = model.predict_proba(val_x)\n",
    "            test_pred = model.predict_proba(test_x)\n",
    "\n",
    "            #特征重要性\n",
    "            # feat_imp_df['imp'] += lgb_model.feature_importance(importance_type='gain') / folds/ len(seeds)\n",
    "            # feat_imp_df = feat_imp_df.sort_values(by='imp', ascending=False).reset_index(drop=True)\n",
    "            # feat_imp_df['rank'] = range(feat_imp_df.shape[0])\n",
    "            \n",
    "            oof[valid_index] = val_pred/ len(seeds)\n",
    "            test_predict += test_pred / kf.n_splits / len(seeds)\n",
    "            \n",
    "            acc_score = accuracy_score(val_y,np.argmax(val_pred,axis=1))\n",
    "            print(acc_score)\n",
    "            acc_scores.append(acc_score)\n",
    "            print('AVG_acc :',sum(acc_scores)/len(acc_scores))\n",
    "            print('std :',np.std(acc_scores))\n",
    "        print('oof:',classification_report(train_y,np.argmax(oof,axis=1)))\n",
    "        \n",
    "    return oof, test_predict,feat_imp_df\n",
    "\n",
    "# 训练ExtraTreesClassifier模型\n",
    "etc_oof, etc_test, lgb_imp_df = lgb_model(train_df[feat_cols].fillna(0), train_y, test_df[feat_cols].fillna(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#xgb\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import StratifiedKFold,KFold\n",
    "from sklearn.metrics import roc_auc_score,accuracy_score,classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def xgb_model(train_x, train_y, test_x):\n",
    "    seeds=[618]\n",
    "    oof = np.zeros([train_x.shape[0], 3])\n",
    "    test_predict = np.zeros([test_x.shape[0], 3])\n",
    "    feat_imp_df = pd.DataFrame()\n",
    "    feat_imp_df['feature'] = train_x.columns\n",
    "    feat_imp_df['imp'] = 0\n",
    "    for seed in seeds:\n",
    "        print('Seed:',seed)\n",
    "        folds = 5\n",
    "        kf = StratifiedKFold(n_splits=folds, shuffle=True, random_state=seed)\n",
    "        acc_scores = []\n",
    "        # train_x = train_x.values\n",
    "        # train_y = train_y.values\n",
    "        \n",
    "        for i, (train_index, valid_index) in enumerate(kf.split(train_x, train_y)):\n",
    "            print(\"|  Model  Fold  {}  Training Start           |\".format(str(i + 1)))\n",
    "            \n",
    "            trn_x, trn_y, val_x, val_y = train_x.values[train_index], train_y[train_index], train_x.values[valid_index], \\\n",
    "                                        train_y[valid_index] \n",
    "            \n",
    "            xgb_params = {\n",
    "                'booster': 'gbtree','objective': 'multi_class','n_estimators':20000,'max_depth': 8,\n",
    "                'lambda': 12,'subsample': 0.7,'num_class':3,'colsample_bytree': 0.8,'colsample_bylevel': 0.7,\n",
    "                'eta': 0.1,'tree_method': 'hist','seed': seed,'nthread': 16,\n",
    "                }\n",
    "            \n",
    "            #训练模型\n",
    "            xgb_model = xgb.XGBClassifier(**xgb_params)\n",
    "            xgb_model.fit(trn_x,trn_y,eval_set=[(trn_x, trn_y),(val_x,val_y)],early_stopping_rounds=100,verbose=1000)\n",
    "            \n",
    "            val_pred  = xgb_model.predict_proba(val_x)\n",
    "            test_pred = xgb_model.predict_proba(test_x)\n",
    "            feat_imp_df['imp'] += xgb_model.feature_importances_ / folds/ len(seeds)\n",
    "            feat_imp_df = feat_imp_df.sort_values(by='imp', ascending=False).reset_index(drop=True)\n",
    "            feat_imp_df['rank'] = range(feat_imp_df.shape[0])\n",
    "            \n",
    "            oof[valid_index] = val_pred / len(seeds)\n",
    "            test_predict += test_pred / kf.n_splits / len(seeds)\n",
    "            \n",
    "            acc_score = accuracy_score(val_y,np.argmax(val_pred,axis=1))\n",
    "            print(acc_score)\n",
    "            acc_scores.append(acc_score)\n",
    "            print('AVG_acc :',sum(acc_scores)/len(acc_scores))\n",
    "        print('oof:',classification_report(train_y,np.argmax(oof,axis=1)))\n",
    "        feat_imp_df.head(25)\n",
    "        \n",
    "    return oof, test_predict,feat_imp_df\n",
    "\n",
    "# 训练 XGB模型\n",
    "xgb_oof, xgb_test, xgb_imp_df = xgb_model(train_df[feat_cols], train_y, test_df[feat_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lgb\n",
    "import warnings\n",
    "import lightgbm as lgb\n",
    "# from lightgbm import early_stopping\n",
    "# from lightgbm import log_evaluation\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import StratifiedKFold,KFold\n",
    "from sklearn.metrics import roc_auc_score,accuracy_score,classification_report,f1_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "def lgb_model(train_x, train_y, test_x):\n",
    "    seeds=[2024]\n",
    "    oof = np.zeros([train_x.shape[0], 3])\n",
    "    test_predict = np.zeros([test_x.shape[0], 3])\n",
    "    feat_imp_df = pd.DataFrame()\n",
    "    feat_imp_df['feature'] = train_x.columns\n",
    "    feat_imp_df['imp'] = 0\n",
    "    for seed in seeds:\n",
    "        print('Seed:',seed)\n",
    "        folds = 12\n",
    "        kf = StratifiedKFold(n_splits=folds, shuffle=True, random_state=seed)\n",
    "        acc_scores = []\n",
    "        # train_x = train_x.values\n",
    "        # train_y = train_y.values\n",
    "        \n",
    "        for i, (train_index, valid_index) in enumerate(kf.split(train_x, train_y)):\n",
    "            print(\"|  Model  Fold  {}  Training Start           |\".format(str(i + 1)))\n",
    "            \n",
    "            trn_x, trn_y, val_x, val_y = train_x.values[train_index], train_y[train_index], train_x.values[valid_index], \\\n",
    "                                        train_y[valid_index] \n",
    "            \n",
    "            lgb_params={\"boosting_type\": \"gbdt\",\"objective\": \"multiclass\",\"metric\": \"multi_logloss\",\n",
    "                        'num_class':3,\"max_depth\": -1,\"learning_rate\": 0.1,\"colsample_bytree\": 0.7,\n",
    "                        \"colsample_bynode\": 0.8,\"verbose\": -1,\"random_state\": seed,\"extra_trees\":True,\n",
    "                        'num_leaves':128,\"verbose\": -1,\"max_bin\":128}\n",
    "            dtrain = lgb.Dataset(trn_x, label=trn_y)\n",
    "            dval = lgb.Dataset(val_x, label=val_y)\n",
    "            lgb_model = lgb.train(lgb_params, dtrain, valid_sets=[dval], num_boost_round=20000,callbacks=[lgb.early_stopping(100), lgb.log_evaluation(500)])\n",
    "\n",
    "            val_pred  = lgb_model.predict(val_x,num_iteration=lgb_model.best_iteration)\n",
    "            test_pred = lgb_model.predict(test_x,num_iteration=lgb_model.best_iteration)\n",
    "\n",
    "            #特征重要性\n",
    "            feat_imp_df['imp'] += lgb_model.feature_importance(importance_type='gain') / folds/ len(seeds)\n",
    "            feat_imp_df = feat_imp_df.sort_values(by='imp', ascending=False).reset_index(drop=True)\n",
    "            feat_imp_df['rank'] = range(feat_imp_df.shape[0])\n",
    "            \n",
    "            oof[valid_index] = val_pred/ len(seeds)\n",
    "            test_predict += test_pred / kf.n_splits / len(seeds)\n",
    "            \n",
    "            acc_score = accuracy_score(val_y,np.argmax(val_pred,axis=1))\n",
    "            print(acc_score)\n",
    "            acc_scores.append(acc_score)\n",
    "            print('AVG_acc :',sum(acc_scores)/len(acc_scores))\n",
    "            print('std :',np.std(acc_scores))\n",
    "        print('oof:',classification_report(train_y,np.argmax(oof,axis=1)))\n",
    "        \n",
    "    return oof, test_predict,feat_imp_df\n",
    "\n",
    "# 训练LGB模型\n",
    "lgb_oof, lgb_test, lgb_imp_df = lgb_model(train_df[feat_cols], train_y, test_df[feat_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#加权融合\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "scores = []; Weights = []\n",
    "best_m = 0\n",
    "best_f1=0\n",
    "for m in np.arange(0.01,1,0.001):\n",
    "    f1 = f1_score(train_y,np.argmax((m*lgb_oof+(1-m)*xgb_oof),axis=1),average='micro')\n",
    "    scores.append(f1)\n",
    "    Weights.append(m)\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        best_m = m\n",
    "# PLOT\n",
    "plt.figure(figsize=(20,5))\n",
    "plt.plot(Weights,scores,'-o',color='blue')\n",
    "plt.scatter([best_m], [best_f1], color='blue', s=300, alpha=1)\n",
    "plt.xlabel('Threshold',size=14)\n",
    "plt.ylabel('Validation F1 Score',size=14)\n",
    "plt.title(f'Best score = {best_f1:.3f} at Best Weights = {best_m:.3}',size=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.242, 0.153, 0.605],\n",
       "       [0.04 , 0.3  , 0.66 ],\n",
       "       [0.023, 0.387, 0.59 ],\n",
       "       ...,\n",
       "       [0.867, 0.035, 0.098],\n",
       "       [0.586, 0.254, 0.16 ],\n",
       "       [0.303, 0.432, 0.265]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "etc_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('./sub/submit_example_A.csv')\n",
    "submission['label'] = pd.DataFrame(np.argmax(etc_test,axis=1))\n",
    "submission.to_csv('./sub/result.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jianyou_exercitation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
