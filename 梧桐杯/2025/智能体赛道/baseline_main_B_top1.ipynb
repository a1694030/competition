{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9167dfa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !unzip /home/jovyan/input/agent/agentA.zip -d /home/jovyan/output/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e9ff0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !honghucli --env=honghu --token=4607c49aeadf4fe3adc74f98e977a8da --type=download --source=agentB.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61f02bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !unzip /home/jovyan/project/agentB.zip -d /home/jovyan/output/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f68fd57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 读取数据\n",
    "# A榜\n",
    "train_df = pd.read_csv(\"/home/jovyan/output/agentA/train.csv\")\n",
    "# test_df = pd.read_csv(\"/home/jovyan/output/agentA/testA.csv\")\n",
    "\n",
    "# B榜\n",
    "test_df = pd.read_csv(\"/home/jovyan/output/agentB/testB.csv\")\n",
    "\n",
    "# 重复值、异常值处理\n",
    "train_df = train_df.drop_duplicates(subset=[col for col in train_df if col not in ['id', 'label']], keep='first')\n",
    "train_df = train_df[~((train_df['innet_dura'] / 12) > train_df['age'])]\n",
    "\n",
    "# 合并训练集，测试集\n",
    "df_all = pd.concat([train_df, test_df], axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b856a53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 流量使用特征\n",
    "df_all['flux_usage_ratio'] = df_all['cm_flux_use'] / (df_all['cm_flux_tot_cnt'] + 1e-6)\n",
    "df_all['base_plan_usage_ratio'] = df_all['cm_base_plan_flux_use'] / (df_all['cm_base_plan_flux'] + 1e-6)\n",
    "df_all['is_over_flux'] = (df_all['out_gprs'] > 0).astype(int)\n",
    "df_all['over_flux_ratio'] = df_all['out_gprs'] / (df_all['cm_flux_use'] + 1e-6)\n",
    "df_all['night_flux_ratio'] = (df_all['wday_night_flux'] + df_all['nwday_night_flux']) / (df_all['cm_flux_use'] + 1e-6)\n",
    "df_all['weekend_flux_ratio'] = (df_all['nwday_day_flux'] + df_all['nwday_night_flux']) / (df_all['cm_flux_use'] + 1e-6)\n",
    "df_all['flux_4g_ratio'] = df_all['flux_4g_use'] / (df_all['cm_flux_use'] + 1e-6)\n",
    "\n",
    "# APP使用特征\n",
    "df_all['video_intensity'] = (df_all['long_vid_use_dur'] + df_all['shrt_vid_use_dur'] + df_all['wtch_liv_use_dur']) / 3600  \n",
    "df_all['realtime_app_intensity'] = (df_all['gm_use_dur'] + df_all['anchor_use_dur'] + df_all['wtch_liv_use_dur']) / 3600\n",
    "df_all['high_bandwidth_score'] = (\n",
    "    df_all['video_cnt_m'] + \n",
    "    df_all['game_cnt_m'] + \n",
    "    df_all['netdisk_use_dur'] / 3600\n",
    ")\n",
    "\n",
    "# 家宽使用活跃度\n",
    "df_all['bd_daily_flux'] = df_all['bd_flux_m'] / (df_all['open_day_m'] + 1e-6)\n",
    "df_all['bd_daily_duration'] = df_all['bd_dur_m'] / (df_all['open_day_m'] + 1e-6)\n",
    "df_all['bd_avg_session_duration'] = df_all['bd_dur_m'] / (df_all['bd_cnt_m'] + 1e-6)\n",
    "df_all['bd_daily_sessions'] = df_all['bd_cnt_m'] / (df_all['open_day_m'] + 1e-6)\n",
    "df_all['arpu_per_flux'] = df_all['arpu'] / (df_all['cm_flux_tot_cnt'] / 1024 + 1e-6)  \n",
    "df_all['high_arpu_heavy_user'] = ((df_all['arpu'] > df_all['arpu'].median()) & \n",
    "                                 (df_all['cm_flux_use'] > df_all['cm_flux_use'].median())).astype(int)\n",
    "df_all['multi_service_user'] = (\n",
    "    df_all['is_bd_tv'] + \n",
    "    df_all['is_fam_vnet_user'] + \n",
    "    df_all['is_ent_vnet_user']\n",
    ")\n",
    "\n",
    "# 用户价值特征\n",
    "df_all['premium_user_potential'] = (\n",
    "    (df_all['arpu'] > df_all['arpu'].quantile(0.7)).astype(int) +\n",
    "    (df_all['video_intensity'] > df_all['video_intensity'].quantile(0.7)).astype(int) +\n",
    "    (df_all['bd_flux_m'] > df_all['bd_flux_m'].quantile(0.7)).astype(int)\n",
    ")\n",
    "df_all['network_strain_indicator'] = (\n",
    "    (df_all['flux_usage_ratio'] > 0.8).astype(int) +  \n",
    "    (df_all['out_gprs'] > 0).astype(int) +  \n",
    "    (df_all['video_intensity'] > df_all['video_intensity'].median()).astype(int)  \n",
    ")\n",
    "df_all['coverage_usage_gap'] = df_all['is_10g_pon'] - (df_all['bd_flux_m'] > df_all['bd_flux_m'].median()).astype(int)\n",
    "df_all['peak_flux_concentration'] = np.maximum(\n",
    "    df_all['wday_day_flux'], \n",
    "    df_all['wday_night_flux']\n",
    ") / (df_all['cm_flux_use'] + 1e-6)\n",
    "\n",
    "flux_columns = ['wday_day_flux', 'wday_night_flux', 'nwday_day_flux', 'nwday_night_flux']\n",
    "df_all['flux_balance_ratio'] = df_all[flux_columns].std(axis=1) / (df_all[flux_columns].mean(axis=1) + 1e-6)\n",
    "df_all['arpu_per_innet_month'] = df_all['arpu'] / (df_all['innet_dura'] + 1e-6)\n",
    "df_all['arpu_per_voice_minute'] = df_all['arpu'] / (df_all['l3m_avg_mou'] + 1e-6)\n",
    "df_all['comprehensive_value_score'] = (\n",
    "    df_all['arpu'] * 0.4 + \n",
    "    df_all['innet_dura'] * 0.3 + \n",
    "    df_all['cm_flux_use'] * 0.3\n",
    ")\n",
    "df_all['plan_utilization_score'] = (\n",
    "    df_all['flux_usage_ratio'] + \n",
    "    df_all['base_plan_usage_ratio'] +\n",
    "    (df_all['out_gprs'] > 0).astype(int) +\n",
    "    (df_all['out_call'] > 0).astype(int)\n",
    ")\n",
    "df_all['customer_life_stage'] = pd.cut(\n",
    "    df_all['innet_dura'], \n",
    "    bins=[0, 12, 36, 60, 120, df_all['innet_dura'].max()],\n",
    "    labels=[1, 2, 3, 4, 5]\n",
    ").astype(int)\n",
    "df_all['mature_user_high_usage'] = (\n",
    "    (df_all['innet_dura'] > 24) & \n",
    "    (df_all['cm_flux_use'] > df_all['cm_flux_use'].median())\n",
    ").astype(int)\n",
    "\n",
    "# 消费潜力特征\n",
    "df_all['loyalty_upgrade_potential'] = (\n",
    "    df_all['innet_dura'] * 0.3 + \n",
    "    df_all['arpu'] * 0.4 + \n",
    "    df_all['plan_utilization_score'] * 0.3\n",
    ")\n",
    "\n",
    "# 整型转换\n",
    "df_all['arpu'] = df_all['arpu'].astype('int')\n",
    "df_all['edu_time_m'] = (df_all['edu_time_m'] > 0).astype('int') \n",
    "df_all['hi_flux_usr_lbl'] = (df_all['hi_flux_usr_lbl'] == 0).astype('int') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093e27b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 类别特征\n",
    "cat_cols = ['is_10g_pon', 'is_bd_status_abnormal', 'is_ent_vnet_user',\n",
    "            'is_fam_vnet_user', 'sev_vid_usr_lbl', 'liv_usr_lbl','is_bd_tv',\n",
    "            'if_high_games_cust', 'vid_usr_lbl', 'read_usr_lbl', 'gm_usr_lbl',\n",
    "            'netdisk_usr_lbl', 'sex', 'if_like_video_cust', 'if_nulim_prod',\n",
    "            'if_like_games_cust','msc_usr_lbl']\n",
    "\n",
    "# 特征编码\n",
    "for col in cat_cols:\n",
    "    df_all[col],_ = df_all[col].factorize()\n",
    "    df_all[col] -= df_all[col].min()\n",
    "    # df_all[col] = df_all[col].astype(\"category\")\n",
    "\n",
    "\n",
    "# 目标编码（KFold 防止泄漏）\n",
    "from sklearn.model_selection import KFold,StratifiedKFold\n",
    "\n",
    "def kfold_target_encode(trn, col, target, n_fold=5, alpha=5):\n",
    "    # 均值编码，alpha 平滑\n",
    "    te = np.zeros(len(trn), dtype='float32')\n",
    "    \n",
    "    global_mean = trn[target].mean()\n",
    "    kf = KFold(n_splits=n_fold, shuffle=True, random_state=42)\n",
    "    \n",
    "    for tr_idx, va_idx in kf.split(trn):\n",
    "        enc = trn.iloc[tr_idx].groupby(col)[target].agg(['mean', 'count'])\n",
    "        smooth = (enc['mean'] * enc['count'] + global_mean * alpha) / (enc['count'] + alpha)\n",
    "        te[va_idx] = trn.iloc[va_idx][col].map(smooth).fillna(global_mean).values\n",
    "    \n",
    "    return te\n",
    "    \n",
    "# 标签\n",
    "target_col = 'label'\n",
    "\n",
    "te_cols = ['arpu', 'arpu_per_flux']\n",
    "for col in te_cols:\n",
    "    te_label = kfold_target_encode(df_all, col=col, target=target_col, n_fold=5, alpha=5)\n",
    "    df_all[f'{col}_te_label'] = te_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43ca10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 交叉统计特征\n",
    "for col in ['bd_flux_m', 'bd_dur_m']:\n",
    "    df_all[f'bd_cnt_m_{col}_mean'] = df_all['bd_cnt_m'].map(df_all.groupby(['bd_cnt_m'])[col].mean())\n",
    "    # df_all[f'bd_cnt_m_{col}_sum'] = df_all['bd_cnt_m'].map(df_all.groupby(['bd_cnt_m'])[col].sum())\n",
    "    # df_all[f'bd_cnt_m_{col}_skew'] = df_all['bd_cnt_m'].map(df_all.groupby(['bd_cnt_m'])[col].skew())\n",
    "\n",
    "# 组合特征\n",
    "from itertools import combinations\n",
    "\n",
    "bd_cols = ['bd_flux_m', 'bd_dur_m', 'bd_cnt_m']\n",
    "\n",
    "for col1, col2 in combinations(bd_cols, 2):\n",
    "    # df_all[f'{col1}+{col2}'] = df_all[col1] + df_all[col2]\n",
    "    # df_all[f'{col1}-{col2}'] = df_all[col1] - df_all[col2]\n",
    "    # df_all[f'{col1}*{col2}'] = df_all[col1] * df_all[col2]\n",
    "    df_all[f'{col1}/{col2}'] = df_all[col1] / df_all[col2].replace(0, np.nan)\n",
    "    \n",
    "df_all[f'{bd_cols[0]}+{bd_cols[1]}+{bd_cols[2]}'] = df_all[bd_cols].sum(axis=1)\n",
    "df_all[f'{bd_cols[0]}*{bd_cols[1]}*{bd_cols[2]}'] = df_all[bd_cols].prod(axis=1)\n",
    "\n",
    "\n",
    "# 特征分箱\n",
    "df_all['arpu_bin'] = pd.qcut(df_all['arpu'], q=5, labels=False, duplicates='drop')\n",
    "df_all['arpu_per_flux_bin'] = pd.qcut(df_all['arpu_per_flux'], q=5, labels=False, duplicates='drop')\n",
    "df_all['cm_flux_tot_cnt_bin'] = pd.qcut(df_all['cm_flux_tot_cnt'], q=5, labels=False, duplicates='drop')\n",
    "df_all['innet_dura_bin'] = pd.qcut(df_all['innet_dura'], q=5, labels=False, duplicates='drop')\n",
    "df_all['arpu_per_innet_month_bin'] = pd.qcut(df_all['arpu_per_innet_month'], q=5, labels=False, duplicates='drop')\n",
    "\n",
    "# 比值特征\n",
    "df_all['flux_arpu_ratio'] = df_all['cm_flux_tot_cnt'] / (df_all['arpu'] + 1)\n",
    "df_all['bd_flux_arpu_ratio'] = df_all['bd_flux_m'] / (df_all['arpu'] + 1)\n",
    "\n",
    "# 特征标准化\n",
    "df_all['arpu_std'] = (df_all['arpu'] - df_all['arpu'].mean()) / df_all['arpu'].std()\n",
    "df_all['flux_std'] = (df_all['cm_flux_tot_cnt'] - df_all['cm_flux_tot_cnt'].mean()) / df_all['cm_flux_tot_cnt'].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1702aeb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分离\n",
    "df_all = df_all.replace([-np.inf, np.inf],0)\n",
    "train_df = df_all[~df_all[target_col].isna()].reset_index(drop=True)\n",
    "test_df = df_all[df_all[target_col].isna()].reset_index(drop=True)\n",
    "\n",
    "adv_feats = ['cm_flux_use', 'flux_4g_use', 'flux_usage_ratio', 'night_flux_ratio',\n",
    "             'weekend_flux_ratio', 'flux_4g_ratio', 'peak_flux_concentration',\n",
    "             'comprehensive_value_score']\n",
    "\n",
    "drop_feats = ['fashion_time_m', 'if_high_games_cust', 'if_like_video_cust', \n",
    "              'cm_chos_plan_flux', 'if_like_games_cust','cm_chos_plan_flux_use']\n",
    "\n",
    "customer_value_feats = ['arpu_bin', 'arpu_per_flux_bin', 'cm_flux_tot_cnt_bin',\n",
    "                        'innet_dura_bin', 'arpu_per_innet_month_bin','arpu_std',\n",
    "                        'flux_arpu_ratio', 'bd_flux_arpu_ratio','flux_std']\n",
    "\n",
    "bd_feats = ['bd_flux_m/bd_dur_m', 'bd_flux_m/bd_cnt_m', 'bd_dur_m/bd_cnt_m', \n",
    "            'bd_flux_m+bd_dur_m+bd_cnt_m', 'bd_flux_m*bd_dur_m*bd_cnt_m']\n",
    "\n",
    "# 训练特征\n",
    "feats1 = [col for col in train_df if col not in ['id', target_col] + adv_feats + drop_feats + customer_value_feats]\n",
    "feats2 = [col for col in train_df if col not in ['id', target_col] + adv_feats + drop_feats + bd_feats]   \n",
    "\n",
    "print('feats1_num:',len(feats1))\n",
    "print('feats2_num:',len(feats2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a880165",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import KFold,StratifiedKFold\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, classification_report\n",
    "warnings.filterwarnings('ignore', category=pd.errors.PerformanceWarning)\n",
    "\n",
    "# 自定义评估函数\n",
    "def c_eval(preds, dtrain):\n",
    "\n",
    "    labels = dtrain.get_label().astype(int)\n",
    "    pred_labels = (preds >= 0.5).astype(int)\n",
    "    acc = accuracy_score(labels, pred_labels)\n",
    "    f1 = f1_score(labels, pred_labels, average='binary')\n",
    "    score = 0.7 * acc + 0.3 * f1\n",
    "    \n",
    "    return 'c_score', score, True\n",
    "   \n",
    "# 评价指标\n",
    "def c_score(y_true, y_pred, average: str = \"binary\"):\n",
    "\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred, average=average)\n",
    "    \n",
    "    return 0.7 * acc + 0.3 * f1   \n",
    "\n",
    "# 模型定义\n",
    "def lgb_mdoel(train, test, target, feats = [], seed = 2025):\n",
    "\n",
    "    score = 0\n",
    "    flods = 5\n",
    "    feat_imp = 0\n",
    "    kf = KFold(n_splits=flods, shuffle=True, random_state=seed)\n",
    "    \n",
    "    oof = np.zeros(len(train))\n",
    "    pred = np.zeros(len(test))\n",
    "    \n",
    "    for i, (train_index, val_index) in enumerate(kf.split(train)):\n",
    "    \n",
    "        print(f\"### LightGBM Fold {i+1}\")\n",
    "        \n",
    "        x_train = train.loc[train_index, feats].copy()\n",
    "        y_train = train.loc[train_index, target].astype(int)    \n",
    "        x_valid = train.loc[val_index, feats].copy()\n",
    "        y_valid = train.loc[val_index, target].astype(int)\n",
    "        x_test = test[feats].copy()\n",
    "\n",
    "        # w_train = compute_age_weights_positive_bias(y_train, x_train['age'], positive_boost=4)\n",
    "        \n",
    "        train_set = lgb.Dataset(x_train, label=y_train, categorical_feature=[])\n",
    "        valid_set = lgb.Dataset(x_valid, label=y_valid, categorical_feature=[], reference=train_set)\n",
    "\n",
    "        # 参数\n",
    "        model_params = {\n",
    "            'boosting_type': 'gbdt',\n",
    "            'objective': 'binary',\n",
    "            'learning_rate': 0.05,\n",
    "            'metric': 'None',\n",
    "            'num_leaves': 64,\n",
    "            'feature_fraction': 0.8,\n",
    "            'bagging_fraction': 0.9,\n",
    "            'bagging_freq': 4,\n",
    "            # 'scale_pos_weight': 2 , # 2.3\n",
    "            'verbose': -1,\n",
    "            'seed': seed,\n",
    "            'n_jobs': 8,\n",
    "      }\n",
    "        \n",
    "        # 训练        \n",
    "        model = lgb.train(\n",
    "            model_params,\n",
    "            train_set,\n",
    "            num_boost_round = 300,\n",
    "            valid_sets = [valid_set],\n",
    "            feval = c_eval,\n",
    "            callbacks = [\n",
    "                # lgb.early_stopping(stopping_rounds=100, verbose=False),\n",
    "                lgb.log_evaluation(period=50)\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        # 预测\n",
    "        pred_val = model.predict(x_valid, num_iteration=model.best_iteration)\n",
    "        pred += model.predict(x_test, num_iteration=model.best_iteration)\n",
    "        oof[val_index] = pred_val\n",
    "\n",
    "        score_kold = c_score(y_valid, (pred_val > 0.5).astype(int))\n",
    "        score += score_kold\n",
    "        print('Score = ',score_kold) \n",
    "\n",
    "        # 特征重要性\n",
    "        feat_imp += model.feature_importance(importance_type='gain') \n",
    "    \n",
    "    pred /= flods\n",
    "    score /= flods\n",
    "    feat_imp /= flods\n",
    "    \n",
    "    print(f\"\\nMean Score  = \", score)\n",
    "    \n",
    "    importance_df = pd.DataFrame({\n",
    "        \"Feature\": feats,\n",
    "        \"Importance\": feat_imp\n",
    "    }).sort_values(by=\"Importance\", ascending=False)\n",
    "\n",
    "    # 垃圾回收\n",
    "    del x_train, y_train, x_valid, y_valid, x_test, train_set, valid_set\n",
    "    del model_params, model\n",
    "    gc.collect()\n",
    "    \n",
    "    return oof, pred, importance_df\n",
    "\n",
    "oof_lgb, pred_lgb, imp_lgb = lgb_mdoel(train_df, test_df, target_col, feats1, seed = 9999) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32cd3180",
   "metadata": {},
   "outputs": [],
   "source": [
    "import catboost\n",
    "from catboost import CatBoostRegressor, CatBoostClassifier, Pool\n",
    "\n",
    "def cat_mdoel(train, test, target, feats = [], categorical_features = [], seed = 2025):\n",
    "\n",
    "    score = 0\n",
    "    flods = 5\n",
    "    feat_imp = 0\n",
    "    skf = StratifiedKFold(n_splits=flods, shuffle=True, random_state=seed)\n",
    "    \n",
    "    oof = np.zeros(len(train))\n",
    "    pred = np.zeros(len(test))\n",
    "    \n",
    "    for i, (train_index, val_index) in enumerate(skf.split(train, train[target])):\n",
    "    \n",
    "        print(f\"### CatBoost Fold {i+1}\")\n",
    "        \n",
    "        x_train = train.loc[train_index, feats].copy()\n",
    "        y_train = train.loc[train_index, target].astype(int)    \n",
    "        x_valid = train.loc[val_index, feats].copy()\n",
    "        y_valid = train.loc[val_index, target].astype(int)\n",
    "        x_test = test[feats].copy()\n",
    "\n",
    "        train_pool = Pool(x_train, y_train, cat_features=categorical_features)\n",
    "        valid_pool = Pool(x_valid, y_valid, cat_features=categorical_features)\n",
    "        test_pool = Pool(x_test, cat_features=categorical_features)\n",
    "        \n",
    "        # 参数\n",
    "        model_params = {\n",
    "                'task_type': 'CPU',\n",
    "                'bootstrap_type': 'Bayesian',\n",
    "                'loss_function': 'Logloss',\n",
    "                'eval_metric': 'AUC',\n",
    "                'learning_rate': 0.1,\n",
    "                'iterations': 800,\n",
    "                # 'depth': 6,\n",
    "                # 'max_leaves': 64,\n",
    "                # 'auto_class_weights': 'Balanced',\n",
    "                # 'grow_policy': 'SymmetricTree',  \n",
    "                'verbose': 100,\n",
    "                # 'early_stopping_rounds': 50,\n",
    "                'random_seed': seed,\n",
    "       }\n",
    "        \n",
    "        # 训练\n",
    "        model = CatBoostClassifier(**model_params)\n",
    "        model.fit(train_pool, eval_set=valid_pool, use_best_model=False)\n",
    "\n",
    "        # 预测\n",
    "        pred_val = model.predict_proba(valid_pool)[:,1]\n",
    "        pred += model.predict_proba(test_pool)[:,1]\n",
    "        oof[val_index] = pred_val\n",
    "\n",
    "        score_kold = c_score(y_valid, (pred_val > 0.5).astype(int))\n",
    "        score += score_kold\n",
    "        print('Score = ',score_kold) \n",
    "        \n",
    "        # 特征重要性\n",
    "        feat_imp += model.feature_importances_\n",
    "    \n",
    "    pred /= flods\n",
    "    score /= flods\n",
    "    feat_imp /= flods\n",
    "    \n",
    "    print(f\"\\nMean Score  = \", score)\n",
    "    \n",
    "    importance_df = pd.DataFrame({\n",
    "        \"Feature\": feats,\n",
    "        \"Importance\": feat_imp \n",
    "    }).sort_values(by=\"Importance\", ascending=False)\n",
    "\n",
    "    # 垃圾回收\n",
    "    del x_train, y_train, x_valid, y_valid, x_test, train_pool, valid_pool, test_pool\n",
    "    del model_params, model\n",
    "    gc.collect()\n",
    "    \n",
    "    return oof, pred, importance_df\n",
    "    \n",
    "categorical_features = [col for col in feats2 if train_df[col].nunique() <= 5] # cat_cols + ['customer_life_stage']\n",
    "oof_cat, pred_cat, imp_cat = cat_mdoel(train_df, test_df, target_col, feats2 , categorical_features, seed = 6666)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0903f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 权重、阈值同步搜索\n",
    "results = [(w, t, c_score(train_df[target_col], ((oof_lgb * w + oof_cat * (1-w)) >= t).astype(int)))\n",
    "            for w in np.arange(0.1, 0.9, 0.05) \n",
    "            for t in np.arange(0.1, 0.9, 0.01)]\n",
    "\n",
    "best_weight, best_threshold, best_score = max(results, key=lambda x: x[2])\n",
    "\n",
    "f_oof = oof_lgb * best_weight + oof_cat * (1 - best_weight)\n",
    "f_pred = pred_lgb * best_weight + pred_cat * (1 - best_weight)\n",
    "\n",
    "print(f\"best_weight: {best_weight:.3f}, best_threshold: {best_threshold:.3f}, best_score: {best_score:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ede6432",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('/home/jovyan/output/agentB/submitB.csv')\n",
    "submission['label'] = (f_pred > best_threshold).astype(int)\n",
    "submission.to_csv('/home/jovyan/output/submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37af59c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!honghucli --env=honghu --token=xxxxxxxxxxxxxxxxxxxx --source=/home/jovyan/output/submission.csv"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
